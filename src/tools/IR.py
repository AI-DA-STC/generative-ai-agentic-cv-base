"""Tool for Image restoration tasks using Restormer"""

from langchain_core.tools import tool

import os
import logging
import json
import sys
import subprocess
import pyprojroot
from typing import List,Optional, Dict

from .decorators import log_io
from src.config import PATHS,PREPROCESSOR_MODEL_MAP,MODEL_SCRIPT_CONFIG

root = pyprojroot.find_root(pyprojroot.has_dir("src"))
sys.path.append(str(root))

logger = logging.getLogger(__name__)

@tool()
@log_io
def create_ir_pipeline(prefix:str,pipeline:Optional[str]) -> str:
    """_summary_

    Args:
        prefix (str): prefix name in minio bucket
        pipeline (Optional[str]): Optional custom pipeline from user to be applied to all images 

    Returns:
        str: summary of the pipeline to be executed
    """
    logger.info("Generating pipeline for image restoration tasks.")
    artefacts_path = f"{PATHS['artefacts']}/{prefix}"
    raw_path = f"{PATHS['raw']}/{prefix}"
    iqa_results_path = os.path.join(artefacts_path, "degredation_iqa_results.json")
    inferred_pipeline = {}
    if not pipeline:
        logger.info(f"No custom pipeline provided by user. Proceed to detect pipeline automatically.")
        if not os.path.exists(iqa_results_path):
            logger.error(f"Degredation IQA results file not found: {iqa_results_path}")
            logger.info(f"Skipping pipeline generation")
        else:
            #mapping the degredations to name of restormer models
            with open(iqa_results_path, 'r') as f:
                iqa_results = json.load(f)
            # Determine which restoration tools to run based on the IQA results
            for fname, degradations in iqa_results.items():
                model_pipeline = {model: [] for model in PREPROCESSOR_MODEL_MAP}
                for degradation in degradations:
                    if degradation.get("severity") not in ("medium", "high", "very high"):
                        continue
                    degradation_type = degradation.get("degradation")
                    for model, mapping in PREPROCESSOR_MODEL_MAP.items():
                        matched_models = mapping.get(degradation_type)
                        if matched_models:
                            model_pipeline[model].extend(matched_models)
                # Remove empty model lists
                inferred_pipeline[fname] = {k: v for k, v in model_pipeline.items() if v}
    else:
        logger.info(f"Custom pipeline provided: {pipeline}")
        for fname in os.listdir(raw_path):
            ext = os.path.splitext(fname)[1].lower()
            if ext not in ['.jpg', '.jpeg', '.png']:
                continue
            model_pipeline = {model: [] for model in PREPROCESSOR_MODEL_MAP}
            for degradation_type in pipeline:
                for model, mapping in PREPROCESSOR_MODEL_MAP.items():
                    matched_models = mapping.get(degradation_type)
                    if matched_models:
                        model_pipeline[model].extend(matched_models)
            inferred_pipeline[fname] = {k: v for k, v in model_pipeline.items() if v}

    # Save the pipeline to a JSON file
    output_pipeline_path = os.path.join(artefacts_path, "preprocessing_pipeline.json")
    with open(output_pipeline_path, 'w', encoding='utf-8') as outfile:
        json.dump(inferred_pipeline, outfile, indent=4)
    logger.info(f"Pipeline generated and saved to {output_pipeline_path}")

    return json.dumps(inferred_pipeline)

@tool()
@log_io
def run_ir_pipeline(prefix: str) -> bool:
    """
    Runs the preprocessing pipeline generated by create_pipeline(), chaining multiple tasks per model,
    keeping only the final output for each image/model, and removing intermediates.
    """
    import shutil

    logger.info("Running pipeline for image restoration tasks.")

    artefacts_path = f"{PATHS['artefacts']}/{prefix}"
    pipeline_path = os.path.join(artefacts_path, "preprocessing_pipeline.json")
    raw_path = f"{PATHS['raw']}/{prefix}"
    processed_path = f"{PATHS['processed']}/{prefix}"
    os.makedirs(processed_path, exist_ok=True)

    if not os.path.exists(pipeline_path):
        logger.error(f"Pipeline file not found: {pipeline_path}. Skipping.")
        return False

    with open(pipeline_path, 'r') as f:
        pipeline = json.load(f)

    results_log = {}

    for filename, model_tasks in pipeline.items():
        original_input = os.path.join(raw_path, filename)
        if not os.path.exists(original_input):
            logger.warning(f"Input image missing: {original_input}")
            continue

        for model_name, task_names in model_tasks.items():
            config = MODEL_SCRIPT_CONFIG.get(model_name)
            if not config:
                logger.warning(f"No config for model: {model_name}")
                continue

            # Start chaining: first input is the raw image
            current_input = original_input
            intermediate_dirs = []

            for task in task_names:
                task_dir = os.path.join(processed_path, model_name, task)
                os.makedirs(task_dir, exist_ok=True)
                intermediate_dirs.append(task_dir)

                output_file = os.path.join(task_dir, filename)
                if model_name == "restormer":
                    cmd = [
                        "conda", "run",
                        "-n", config["env"],
                        config["python"],
                        config["script"],
                        "--task", task,
                        "--input_dir", current_input,
                        "--result_dir", task_dir
                    ]
                elif model_name == "swinir":
                    cmd = [
                        "conda", "run",
                        "-n", config["env"],
                        config["python"],
                        config["script"],
                        "--task", task,
                        "--folder_lq", current_input,
                        "--save_dir", task_dir,
                    ]
                elif model_name == "xrestormer":
                    cmd = [
                        "conda", "run",
                        "-n", config["env"],
                        config["python"],
                        config["script"],
                        "-opt", f"{root}/configs/xrestormer/{task}.yml",
                    ]
                else:
                    logger.error(f"Unknown model: {model_name}")
                    continue

                logger.info(f"Running: {' '.join(cmd)}")
                try:
                    res = subprocess.run(cmd, check=True, capture_output=True, text=True)
                    logger.info(f"{model_name}/{task} succeeded for {filename}")
                    results_log.setdefault(filename, {}).setdefault(model_name, []).append({
                        "task": task,
                        "status": "success",
                        "stdout": res.stdout
                    })
                    # next input is this task's output
                    current_input = output_file
                except subprocess.CalledProcessError as e:
                    logger.error(f"Error {model_name}/{task} on {filename}: {e.stderr}")
                    results_log.setdefault(filename, {}).setdefault(model_name, []).append({
                        "task": task,
                        "status": "error",
                        "stderr": e.stderr
                    })
                    # abort further tasks for this model
                    break

            # After chaining, move final result to model-specific folder
            final_dir = os.path.join(processed_path, model_name)
            os.makedirs(final_dir, exist_ok=True)
            if os.path.exists(current_input):
                shutil.move(current_input, os.path.join(final_dir, filename))

            # Clean up intermediate directories
            for d in intermediate_dirs:
                if os.path.isdir(d):
                    shutil.rmtree(d)

    # Save log
    log_path = os.path.join(artefacts_path, "run_pipeline_log.json")
    with open(log_path, 'w') as lf:
        json.dump(results_log, lf, indent=4)

    return True
